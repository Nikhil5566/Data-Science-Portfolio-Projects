# -*- coding: utf-8 -*-
"""Indian Smartphones Under ₹20,000–Specs, Prices Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Zk4j7Np3vwIvddkV-S54XuufObK5uiWV

# Indian Smartphones Under ₹20,000–Specs, Prices Dataset

# Introduction & Insights

---

Decoding the Budget Smartphone Market: A Data-Driven Analysis
The Dataset The Indian smartphone market, particularly the under-₹20,000 segment, is a chaotic jungle of specifications. Our project utilizes a scraped dataset of hundreds of devices, capturing critical features such as RAM Capacity, Internal Memory, Battery Size, User Ratings, and Price.

Initially, this data presented a classic "real-world" challenge: it was messy. Prices were trapped in text strings ("₹12,999"), and technical specs were cluttered with units ("6 GB", "5000 mAh"). Before any analysis could begin, we engineered a rigorous Data Cleaning Pipeline. By stripping currency symbols and standardizing units into pure numerical values, we transformed a raw, unreadable text file into a high-quality analytical asset ready for statistical interrogation.

Statistical Insights & Market Truths Once cleaned, the data revealed the hidden physics of smartphone pricing. We moved beyond simple averages to test specific hypotheses about value and cost:

The "Performance Tax" (Correlation): Our correlation analysis highlighted that RAM Capacity is often the single strongest driver of price in this segment. While storage matters, the market clearly charges a premium for speed (6GB vs. 8GB) more aggressively than for other features.

The "Battery Paradox" (T-Test): Contrary to popular belief, a bigger battery does not always equal a higher price. Our statistical tests typically reveal that massive 6000mAh batteries are often features of budget devices (sacrificing slimness for power), whereas more expensive "premium" budget phones often stick to standard 5000mAh cells to maintain a sleek design.

The "Brand Premium" (ANOVA): By grouping prices by Brand Name, we quantified the "Brand Tax." The analysis determines whether a user pays significantly more for a Samsung device compared to a Poco device with identical specifications. This insight is crucial for consumers seeking pure "value for money" versus those paying for brand reputation.

Predictive Capability The pinnacle of this project is the Price Prediction Model. By training a Random Forest Regressor on the cleaned specification data, we built an AI capability that can estimate the fair market price of a phone based solely on its specs. This transforms the project from a historical report into a forward-looking tool, capable of spotting "overpriced" devices (where the actual price exceeds the AI's prediction) or "hidden gems" (where the price is lower than the specs suggest).


---
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
import warnings
warnings.filterwarnings('ignore')

# Installing Kaggle API
!pip install -q kaggle

# Uploading kaggle.json
from google.colab import files
files.upload()

# Moving kaggle.json to ~/.kaggle/
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Downloading the dataset using Kaggle API
!kaggle datasets download -d demonllord/indian-smartphones-under-20000specs-prices

# Unziping the downloaded file
!unzip indian-smartphones-under-20000specs-prices.zip

"""# Data Inspection"""

df = pd.read_csv('indian-smartphones-under-20000/data/indian_smartphones_under_20000_raw.csv')
df.head(11)

df.tail(11)

"""# Dropping the Unnamed column"""

if 'Unnamed: 0' in df.columns:
    df = df.drop(columns=['Unnamed: 0'])
    print("'Unnamed: 0' column dropped.")
else:
    print("'Unnamed: 0' column not found.")

df.shape

df.columns

df.dtypes

df.info()

df.describe(include = 'all').T

df.isnull().sum()

df.duplicated().sum()

import scipy.stats as stats
import re
import numpy as np

print("\n--- Market Insights (Statistical Proof) ---")

# --- 1. Data Preparation (Extracting Numeric Features) ---
# We need to extract numbers from text columns to perform statistics

# Extract RAM (e.g., "8 GB RAM" -> 8)
def extract_ram(text):
    match = re.search(r'(\d+)\s*GB\s*RAM', str(text), re.IGNORECASE)
    return int(match.group(1)) if match else None

# Extract Battery (e.g., "5000 mAh" -> 5000)
def extract_battery(text):
    match = re.search(r'(\d+)\s*mAh', str(text), re.IGNORECASE)
    return int(match.group(1)) if match else None

# Extract Brand (First word of Model Name)
def extract_brand(text):
    return str(text).split()[0] if isinstance(text, str) else "Unknown"

# Apply extractions if columns don't exist yet
if 'RAM' not in df.columns:
    df['RAM'] = df['RAM & ROM'].apply(extract_ram)

if 'Battery_Capacity_mAh' not in df.columns:
    df['Battery_Capacity_mAh'] = df['Battery Capacity'].apply(extract_battery)

if 'Brand' not in df.columns:
    df['Brand'] = df['Model Name'].apply(extract_brand)

# --- 2. Statistical Analysis ---

# Insight 1: The "Spec-to-Price" Correlation
# Using corrected column names: 'Price', 'RAM', 'Battery_Capacity_mAh', 'Ratings'
print("\nWhat drives the Price? (Correlation):")
corr = df[['Price', 'RAM', 'Battery_Capacity_mAh', 'Ratings']].corr()
print(corr['Price'].sort_values(ascending=False))

# Insight 2: The "Brand Tax" (ANOVA Test)
# Using 'Brand' instead of 'brand_name'
top_brands = df['Brand'].value_counts().head(5).index
brand_prices = [df[df['Brand'] == b]['Price'].dropna() for b in top_brands]

f_stat, p_val = stats.f_oneway(*brand_prices)

print(f"\nDoes Brand Name affect Price? (ANOVA Test)")
print(f"P-Value: {p_val:.5f}")
if p_val < 0.05:
    print("✅ YES. There is a significant 'Brand Premium' in this market.")
else:
    print("❌ NO. All top brands are priced competitively.")

# Insight 3: The "Battery Value" Test (T-Test)
# Do 6000mAh phones cost significantly more than 5000mAh phones?
bat_5000 = df[df['Battery_Capacity_mAh'] == 5000]['Price'].dropna()
bat_6000 = df[df['Battery_Capacity_mAh'] == 6000]['Price'].dropna()

if len(bat_5000) > 1 and len(bat_6000) > 1:
    t_stat, p_val_bat = stats.ttest_ind(bat_5000, bat_6000, equal_var=False)

    print(f"\nDo bigger batteries cost more? (T-Test)")
    print(f"Avg Price (5000mAh): ₹{bat_5000.mean():.0f}")
    print(f"Avg Price (6000mAh): ₹{bat_6000.mean():.0f}")
    if p_val_bat < 0.05:
        print("✅ YES. You pay a premium for more battery.")
    else:
        print("❌ NO. 6000mAh batteries are often in cheaper phones!")
else:
    print("\nNot enough data to compare 5000mAh vs 6000mAh prices.")

"""# Univariate Analysis"""

# --- 1. Numerical Columns (Histograms in a Grid) ---
numerical_cols = df.select_dtypes(include=np.number).columns.tolist()

if len(numerical_cols) > 0:
    # Define grid size
    num_plots = len(numerical_cols)
    num_cols = 3  # You can change this to 2 or 4 if preferred
    num_rows = math.ceil(num_plots / num_cols)

    # Create figure and axes
    fig, axes = plt.subplots(num_rows, num_cols, figsize=(5 * num_cols, 4 * num_rows))
    axes = axes.flatten()  # Flatten to 1D array for easy iteration

    # Iterate and plot
    for i, col in enumerate(numerical_cols):
        sns.histplot(data=df, x=col, kde=True, ax=axes[i])
        axes[i].set_title(f'Distribution of {col}')
        axes[i].set_xlabel(col)
        axes[i].set_ylabel('Frequency')

    # Hidding any unused subplots (if total plots < grid cells)
    for j in range(i + 1, len(axes)):
        axes[j].axis('off')

    plt.tight_layout()
    plt.show()
else:
    print("No numerical columns found.")


# --- 2. Categorical Columns (Countplots in a Grid) ---
categorical_cols = df.select_dtypes(include='object').columns.tolist()

if len(categorical_cols) > 0:
    # Define grid size
    num_plots = len(categorical_cols)
    num_cols = 3
    num_rows = math.ceil(num_plots / num_cols)

    # Create figure and axes
    fig, axes = plt.subplots(num_rows, num_cols, figsize=(5 * num_cols, 4 * num_rows))
    axes = axes.flatten()

    # Iterate and plot
    for i, col in enumerate(categorical_cols):
        sns.countplot(data=df, x=col, ax=axes[i])
        axes[i].set_title(f'Distribution of {col}')
        axes[i].set_xlabel(col)
        axes[i].set_ylabel('Count')
        axes[i].tick_params(axis='x', rotation=45)

    # Hide unused subplots
    for j in range(i + 1, len(axes)):
        axes[j].axis('off')

    plt.tight_layout()
    plt.show()

    # --- 3. Value Counts (Text Output) ---
    print("--- Categorical Value Counts ---")
    for col in categorical_cols:
        print(f"\nValue counts for {col}:")
        print(df[col].value_counts())
else:
    print("No categorical columns found.")

"""# Bivariate Analysis"""

import itertools
import math


# --- 1. Numerical-Numerical Relationships (Scatter Plots) ---
numerical_cols = df.select_dtypes(include=np.number).columns.tolist()

# Generate all unique pairs of numerical columns
num_pairs = list(itertools.combinations(numerical_cols, 2))

if len(num_pairs) > 0:
    # Define grid size
    num_plots = len(num_pairs)
    num_cols = 3
    num_rows = math.ceil(num_plots / num_cols)

    # Create figure and axes
    fig, axes = plt.subplots(num_rows, num_cols, figsize=(5 * num_cols, 4 * num_rows))

    # Flatten axes for easy iteration.
    # Handle edge case where there is only 1 plot (axes is not an array)
    if num_plots == 1:
        axes = [axes]
    else:
        axes = axes.flatten()

    # Iterate through pairs and plot
    for i, (col1, col2) in enumerate(num_pairs):
        sns.scatterplot(data=df, x=col1, y=col2, ax=axes[i])
        axes[i].set_title(f'{col1} vs {col2}')
        axes[i].set_xlabel(col1)
        axes[i].set_ylabel(col2)

    # Hide unused subplots
    for j in range(i + 1, len(axes)):
        axes[j].axis('off')

    plt.tight_layout()
    plt.show()
else:
    print("Not enough numerical columns for scatter plots.")


# --- 2. Categorical-Numerical Relationships (Box Plots) ---
categorical_cols = df.select_dtypes(include='object').columns.tolist()
numerical_cols = df.select_dtypes(include=np.number).columns.tolist()

# Generate all pairs of (Categorical, Numerical)
cat_num_pairs = list(itertools.product(categorical_cols, numerical_cols))

if len(cat_num_pairs) > 0:
    # Define grid size
    num_plots = len(cat_num_pairs)
    num_cols = 3
    num_rows = math.ceil(num_plots / num_cols)

    # Create figure and axes
    fig, axes = plt.subplots(num_rows, num_cols, figsize=(5 * num_cols, 4 * num_rows))

    if num_plots == 1:
        axes = [axes]
    else:
        axes = axes.flatten()

    # Iterate through pairs and plot
    for i, (cat_col, num_col) in enumerate(cat_num_pairs):
        # 1. Get top 10 categories based on frequency
        top_categories = df[cat_col].value_counts().nlargest(10).index.tolist()

        # 2. Filter data: Keep only top categories AND ensure numerical column is not NaN
        filtered_df = df[df[cat_col].isin(top_categories) & df[num_col].notnull()]

        # 3. Update 'order' to only include categories that exist in the filtered data
        # This prevents the ValueError where a category in 'order' has no data to plot
        valid_categories = [cat for cat in top_categories if cat in filtered_df[cat_col].unique()]

        if valid_categories:
            sns.boxplot(data=filtered_df, x=cat_col, y=num_col, ax=axes[i], order=valid_categories)
            axes[i].set_title(f'{num_col} by {cat_col} (Top 10)')
            axes[i].set_xlabel(cat_col)
            axes[i].set_ylabel(num_col)
            axes[i].tick_params(axis='x', rotation=45)
        else:
             axes[i].text(0.5, 0.5, "No valid data", ha='center', va='center')

    # Hide unused subplots
    for j in range(i + 1, len(axes)):
        axes[j].axis('off')

    plt.tight_layout()
    plt.show()
else:
    print("No categorical-numerical pairs found.")

"""# Pairplot"""

sns.pairplot(df.select_dtypes(include = np.number))
plt.suptitle('Pairplot of Numerical Features', y=1.02)
plt.show()

"""# Heatmap"""

plt.figure(figsize=(15, 8))
sns.heatmap(df.corr(numeric_only = True), annot=True, cmap="coolwarm", linewidths=0.5)
plt.title("Correlation Matrix", fontsize=14)
plt.show()

"""# Statistical Analysis"""

import scipy.stats as stats
import matplotlib.pyplot as plt
import seaborn as sns

# Fix: Create 'Is_5G' column by extracting it from the Model Name
# The error occurred because 'Is_5G' didn't exist in the dataframe
if 'Is_5G' not in df.columns:
    df['Is_5G'] = df['Model Name'].astype(str).apply(lambda x: 1 if '5G' in x.upper() else 0)

# T-Test: Average 'Price' of 5G vs. non-5G smartphones
print("--- T-Test (Price of 5G vs. non-5G Smartphones) ---")

# Separate prices for 5G and non-5G phones, dropping NaN prices
price_5g = df[df['Is_5G'] == 1]['Price'].dropna()
price_non_5g = df[df['Is_5G'] == 0]['Price'].dropna()

# Check if there's enough data for both groups
if len(price_5g) > 1 and len(price_non_5g) > 1:
    # Perform independent t-test
    t_statistic, p_value_ttest = stats.ttest_ind(price_5g, price_non_5g, equal_var=False) # Assuming unequal variances

    print(f"Average Price (5G): \u20b9{price_5g.mean():.2f}")
    print(f"Average Price (Non-5G): \u20b9{price_non_5g.mean():.2f}")
    print(f"T-statistic: {t_statistic:.2f}")
    print(f"P-value: {p_value_ttest:.4f}")

    if p_value_ttest < 0.05:
        print("Conclusion: There is a statistically significant difference in average prices between 5G and non-5G smartphones.")
    else:
        print("Conclusion: There is no statistically significant difference in average prices between 5G and non-5G smartphones.")

    # Visualize the price distribution for 5G vs non-5G
    plt.figure(figsize=(8, 6))
    sns.boxplot(x='Is_5G', y='Price', data=df.dropna(subset=['Price', 'Is_5G']), palette='coolwarm')
    plt.title('Price Distribution: 5G vs. Non-5G Smartphones', fontsize=16)
    plt.xlabel('Is 5G (0 = No, 1 = Yes)', fontsize=12)
    plt.ylabel('Price', fontsize=12)
    plt.xticks(ticks=[0, 1], labels=['Non-5G', '5G'])
    plt.tight_layout()
    plt.show()
else:
    print("Not enough data in one or both groups to perform the t-test.")

"""# Feature Engineering"""

import re

print("\n--- Starting Feature Engineering (Derived Metrics and Categories) ---")

# 1. Cleaning 'Percentage Discount' if it's still an object
if 'Percentage Discount' in df.columns and df['Percentage Discount'].dtype == 'object':
    def clean_percentage_discount(x):
        if isinstance(x, str):
            # Remove '%' sign and any non-digit characters, then convert to float
            return float(re.sub(r'[^\d.]', '', x.replace('%', '')))
        return x
    df['Percentage Discount'] = df['Percentage Discount'].apply(clean_percentage_discount)
    print("Cleaned column: Percentage Discount")

# FIX: Ensure 'ROM' (Storage) column exists before using it
if 'ROM' not in df.columns:
    def extract_rom(text):
        # Extract number before "GB ROM"
        match = re.search(r'(\d+)\s*GB\s*ROM', str(text), re.IGNORECASE)
        if match:
            return int(match.group(1))
        # Extract number before "TB ROM" and convert to GB
        match_tb = re.search(r'(\d+)\s*TB\s*ROM', str(text), re.IGNORECASE)
        if match_tb:
            return int(match_tb.group(1)) * 1024
        return None

    df['ROM'] = df['RAM & ROM'].apply(extract_rom)
    print("Created extracted column: 'ROM'")

# 2. CREATING RATIO FEATURES (The "Value" Metrics)
# Price per GB of RAM (The "Performance Tax") - Re-calculating to ensure consistency
# Ensure RAM exists
if 'RAM' not in df.columns:
    def extract_ram(text):
        match = re.search(r'(\d+)\s*GB\s*RAM', str(text), re.IGNORECASE)
        return int(match.group(1)) if match else None
    df['RAM'] = df['RAM & ROM'].apply(extract_ram)

df['Price_per_GB_RAM'] = df['Price'] / df['RAM'].replace({0:1}).fillna(1)

# Storage to RAM Ratio (Balance Check)
# Now 'ROM' is guaranteed to exist
df['storage_ram_ratio'] = df['ROM'].fillna(0) / df['RAM'].replace({0:1}).fillna(1)

# Battery Value (Juice per Rupee)
df['battery_value_score'] = df['Battery_Capacity_mAh'].fillna(0) / df['Price'].replace({0:1}).fillna(1)

# 3. BRAND ENCODING (Average Price of the Brand)
# Use 'Brand' column (already extracted) and 'Price' column (already numeric).
if 'Brand' not in df.columns:
     df['Brand'] = df['Model Name'].apply(lambda x: str(x).split()[0])

brand_price_map = df.groupby('Brand')['Price'].mean().to_dict()
df['brand_avg_price'] = df['Brand'].map(brand_price_map)

# 4. PRICE CATEGORY (Binning)
df['price_segment'] = pd.cut(df['Price'],
                             bins=[0, 8000, 12000, 16000, 25000],
                             labels=['Entry-Level', 'Budget', 'Mid-Range', 'Premium-Budget'])

print("Feature Engineering Complete.")
print(f"New Data Shape: {df.shape}")
print(df[['Brand', 'Price', 'Price_per_GB_RAM', 'brand_avg_price', 'storage_ram_ratio', 'battery_value_score', 'price_segment']].head())

# Visualize the new "Value" metric (Price per GB of RAM)
plt.figure(figsize=(12, 8))
sns.barplot(data=df.dropna(subset=['Brand', 'Price_per_GB_RAM']), x='Brand', y='Price_per_GB_RAM', palette='magma')
plt.title('Which Brand charges the most per GB of RAM?', fontsize=16)
plt.xticks(rotation=45)
plt.ylabel('Price per GB (₹)', fontsize=12)
plt.xlabel('Brand', fontsize=12)
plt.tight_layout()
plt.show()

"""# Clustering"""

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import seaborn as sns
import matplotlib.pyplot as plt

print("\n--- Starting Market Segmentation (Clustering) ---")

# 1. Select Features for Clustering
# We want to group phones based on Price and Specs
cluster_features = ['Price', 'RAM', 'Battery_Capacity_mAh', 'Ratings']

# Handle missing values (if any)
df_cluster = df[cluster_features].dropna()

# 2. Scale the Data (Crucial for K-Means)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_cluster)

# 3. Apply K-Means Clustering
# We'll assume 3 distinct market segments for now
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10) # Added n_init for newer sklearn versions
df_cluster['Segment'] = kmeans.fit_predict(X_scaled)

# 4. Interpret the Segments
# Group by the new 'Segment' column and see the average stats
segment_analysis = df_cluster.groupby('Segment').mean().sort_values('Price')
print("\nMarket Segments Identified (Average Stats):")
print(segment_analysis)

# 5. Visualizing the Segments
# Plot Price vs RAM, colored by Segment
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df_cluster, x='Price', y='RAM',
                hue='Segment', palette='viridis', s=100, alpha=0.8)
plt.title('Smartphone Market Segments: Price vs RAM')
plt.xlabel('Price (₹)')
plt.ylabel('RAM (GB)')
plt.grid(True, alpha=0.3)
plt.show()

# 6. Identifying "Best Value" Outliers
# Phones that have High Specs but Low Price (Bottom Right of the chart)
print("\n--- Potential 'Hidden Gems' (High RAM, Low Price) ---")
# Filter: Segment with lowest price but RAM > 4GB
# Note: This is an example, segment interpretation might vary
value_phones = df_cluster[(df_cluster['Price'] < df_cluster['Price'].mean()) & (df_cluster['RAM'] >= df_cluster['RAM'].median())]
if not value_phones.empty:
    print(value_phones.head())
else:
    print("No obvious outliers found based on the defined criteria.")

"""# Machine learning"""

import re
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score

print("\n--- 1. Data Cleaning & Preparation ---")

# A. Smart Column Finder (Fixes the KeyError)
# Finds any column that contains the word "price" (case insensitive)
try:
    price_col = [c for c in df.columns if 'price' in c.lower()][0]
    print(f"✅ Found Target Column: '{price_col}'")
except IndexError:
    raise ValueError("Could not find a 'Price' column. Please check df.columns!")

# B. Clean Currency (Remove thousands separators like ',', then convert to float)
def clean_currency(x):
    if isinstance(x, str):
        # Remove commas (thousands separator)
        cleaned_x = x.replace(',', '')

        # Handling multiple periods: assume all but the last are thousands separators
        # E.g., '3.811.5' should become '3811.5'
        if cleaned_x.count('.') > 1:
            parts = cleaned_x.split('.')
            cleaned_x = ''.join(parts[:-1]) + '.' + parts[-1]

        # Remove any non-numeric characters that might remain (except for single dot)
        clean_str = re.sub(r'[^\d.]', '', cleaned_x)

        return float(clean_str) if clean_str else np.nan
    return x

# Applying cleaning to Price and likely numeric columns
df['cleaned_price'] = df[price_col].apply(clean_currency)

# Clean other specs if they are strings (e.g., "4 GB" -> 4.0)
for col in df.columns:
    if 'ram' in col.lower() or 'battery' in col.lower() or 'memory' in col.lower():
        if df[col].dtype == 'object':
            df[col] = df[col].apply(clean_currency)

# Drop rows where Price is missing
df_ml = df.dropna(subset=['cleaned_price'])

# ==========================================
# STEP 6: MODEL TRAINING
# ==========================================
print("\n--- 2. Training Price Prediction Model ---")

# Define Features (X) and Target (y)
# We drop the original dirty price and text columns we can't use yet
X = df_ml.drop([price_col, 'cleaned_price'], axis=1)
# Select only numeric features for simplicity (plus One-Hot Encoded Brands)
X = X.select_dtypes(include=[np.number])

# Add Brand Name via One-Hot Encoding (Crucial for pricing!)
# Ensure 'Brand' column exists before trying to one-hot encode it
if 'Brand' in df_ml.columns:
    dummies = pd.get_dummies(df_ml['Brand'], prefix='Brand', drop_first=True)
    X = pd.concat([X, dummies], axis=1)

y = df_ml['cleaned_price']

# Split Data (80% Train, 20% Test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Random Forest
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# ==========================================
# STEP 7: EVALUATION & INSIGHTS
# ==========================================
preds = model.predict(X_test)
mae = mean_absolute_error(y_test, preds)
r2 = r2_score(y_test, preds)

print(f"\n--- Model Results ---")
print(f"Model Accuracy (R²): {r2:.2%} (Explains {r2:.2%} of price variation)")
print(f"Average Error (MAE): ₹{mae:.0f}")

"""# Executive Summary: The Truth About Smartphone Pricing

Through rigorous statistical analysis and machine learning, we have decoded the pricing strategy of the Indian smartphone market (Under ₹20,000).

### 1. The "Brand Tax" is Real
Our **ANOVA Test (p < 0.05)** confirms that brand identity significantly dictates price. A user pays a premium for established Tier-1 brands compared to value-focused challengers, even when specifications are identical.

### 2. The "5G Premium"
There is a distinct pricing tier for 5G connectivity. Our **T-Test** revealed that 5G devices cost significantly more (**avg. ~₹13k**) than their 4G counterparts (**avg. ~₹8.8k**). 5G is currently the most expensive "feature" you can buy.

### 3. The "Battery Paradox"
Contrary to intuition, a larger battery does not always mean a higher price. **SHAP analysis** and correlation matrices show that massive **6000mAh+ batteries** are often found in budget devices (trading design for power), while premium-budget phones often stick to 5000mAh to maintain slim aesthetics.

### 4. Performance is the Main Driver
**RAM Capacity** has the strongest positive correlation with price. The market charges linearly for performance (speed), making it the most reliable predictor of a phone's cost.
"""

# Plot: Actual vs Predicted
plt.figure(figsize=(10, 6))
sns.scatterplot(x=y_test, y=preds, alpha=0.7, color='purple')
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2) # Perfect prediction line
plt.title(f'Actual vs Predicted Price (MAE: ₹{mae:.0f})')
plt.xlabel('Actual Price')
plt.ylabel('Predicted Price')
plt.show()

# Plot: Feature Importance (What drives the price?)
importances = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False).head(10)
plt.figure(figsize=(10, 6))
importances.plot(kind='barh', color='teal')
plt.title('Top 10 Features Determining Smartphone Price')
plt.gca().invert_yaxis()
plt.show()

!pip install -q shap

# --- Professional Enhancement 1: NLP Feature Extraction (Colors) ---
import re
import matplotlib.pyplot as plt
import seaborn as sns

# List of common smartphone colors to search for
common_colors = ['Black', 'Blue', 'Gold', 'Silver', 'White', 'Red', 'Green', 'Grey', 'Gray', 'Purple', 'Yellow', 'Orange']

def extract_color(text):
    text = str(text).title()
    for color in common_colors:
        if color in text:
            return color
    return 'Other'

# Apply extraction
df['Color'] = df['Model Name'].apply(extract_color)

# Handle 'Grey' vs 'Gray'
df['Color'] = df['Color'].replace({'Gray': 'Grey'})

print("--- Color Extraction Analysis ---")
print(df['Color'].value_counts().head(5))

# Visualize: Does Color Affect Price?
plt.figure(figsize=(12, 6))
# Filter for main colors and remove rare ones for cleaner plot
top_colors = df['Color'].value_counts().head(8).index
sns.boxplot(data=df[df['Color'].isin(top_colors)], x='Color', y='Price', palette='hsv')
plt.title('Do Smartphone Colors Affect Price?', fontsize=16)
plt.xlabel('Color')
plt.ylabel('Price')
plt.show()

"""# Model Explainability with SHAP"""

# --- Professional Enhancement 2: Model Explainability with SHAP ---
import shap

# Initialize JavaScript visualizations
shap.initjs()

# We use the TreeExplainer for Random Forest (Optimized for trees)
explainer = shap.TreeExplainer(model)

# Calculate SHAP values for the Test set (using a subset to speed it up if data is huge)
# X_test is already defined from our ML section
shap_values = explainer.shap_values(X_test)

print("\n--- SHAP Summary Plot ---")
print("This plot shows the 'Impact' of each feature on the Price.")
print("Red dots = High feature value (e.g., High RAM), Blue dots = Low feature value.")
print("If Red dots are on the RIGHT, it means that feature INCREASES the price.")

# Summary Plot
plt.figure(figsize=(10, 8))
shap.summary_plot(shap_values, X_test, plot_type="dot")

print("\n--- Interpretation ---")
print("1. Look at 'RAM': You should see Red dots (High RAM) pushing to the right (Higher Price).")
print("2. Look at 'Battery': Surprisingly, sometimes Red dots (Big Battery) might push to the left if budget phones have huge batteries.")

"""# Conclusion

This project bridges the gap between raw web-scraped data and actionable consumer intelligence. It proves that in the crowded budget market, price is not random—it is a calculable sum of performance, brand power, and strategic trade-offs.

# Thank You
"""